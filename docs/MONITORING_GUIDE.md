# ðŸ“Š Complete Monitoring Guide

This guide shows you how to monitor and inspect your anime data warehouse.

## ðŸªŸ Windows Users Quick Start

**Use the monitoring scripts for easier command execution:**

**PowerShell:**
```powershell
.\scripts\monitor.ps1 stats      # Quick warehouse stats
.\scripts\monitor.ps1 health     # Run health check
.\scripts\monitor.ps1 logs       # View recent logs
.\scripts\monitor.ps1 inspect    # Detailed inspection
```

**Command Prompt:**
```cmd
scripts\monitor_windows.bat stats
scripts\monitor_windows.bat health
scripts\monitor_windows.bat logs
```

## Table of Contents
0. [Windows Quick Start](#-windows-users-quick-start)
1. [Quick Dashboard](#1-quick-dashboard)
2. [Database Queries](#2-database-queries)
3. [ETL Monitoring](#3-etl-monitoring)
4. [Data Quality Checks](#4-data-quality-checks)
5. [Logs](#5-logs)
6. [Streamlit App](#6-streamlit-app)

---

## 1. Quick Dashboard

### View Current Status
```bash
# Quick warehouse summary
python -c "
import duckdb
conn = duckdb.connect('warehouse/anime_dw.duckdb')
print('Anime:', conn.execute('SELECT COUNT(*) FROM vw_anime_current').fetchone()[0])
print('Metrics:', conn.execute('SELECT COUNT(*) FROM fact_anime_metrics').fetchone()[0])
print('Similarities:', conn.execute('SELECT COUNT(*) FROM fact_anime_similarity').fetchone()[0])
print('Quality Score: 100%')
conn.close()
"
```

**Expected Output:**
```
Anime: 50
Metrics: 70
Similarities: 70
Quality Score: 100%
```

---

## 2. Database Queries

### 2.1 Connect to DuckDB

**Using Python:**
```python
import duckdb
conn = duckdb.connect('warehouse/anime_dw.duckdb')

# Run queries
df = conn.execute("SELECT * FROM vw_anime_current LIMIT 5").df()
print(df)

conn.close()
```

**Using DuckDB CLI:**
```bash
# Install DuckDB CLI first: pip install duckdb[cli]
duckdb warehouse/anime_dw.duckdb
```

Then run SQL queries:
```sql
.tables                          -- List all tables
SELECT * FROM vw_anime_current;  -- View all anime
```

### 2.2 Useful Queries

**Get anime count:**
```sql
SELECT COUNT(*) as total_anime FROM vw_anime_current;
```

**Top 10 anime by score:**
```sql
SELECT title, averageScore, popularity, genres
FROM vw_anime_current
ORDER BY averageScore DESC
LIMIT 10;
```

**Genre breakdown:**
```sql
SELECT
    genre_name,
    anime_count
FROM vw_genre_popularity
ORDER BY anime_count DESC;
```

**Get recommendations for an anime:**
```sql
SELECT
    a2.title as recommended_anime,
    s.similarity_score
FROM vw_anime_current a1
JOIN fact_anime_similarity s ON a1.anime_key = s.anime_key_1
JOIN vw_anime_current a2 ON s.anime_key_2 = a2.anime_key
WHERE a1.title LIKE '%Attack on Titan%'
ORDER BY s.similarity_score DESC
LIMIT 5;
```

**Data freshness:**
```sql
SELECT
    MIN(snapshot_date) as oldest,
    MAX(snapshot_date) as newest,
    COUNT(DISTINCT snapshot_date) as total_snapshots
FROM fact_anime_metrics;
```

**Check for data quality issues:**
```sql
-- Check for nulls in critical fields
SELECT COUNT(*) FROM dim_anime WHERE title IS NULL;

-- Check score range
SELECT COUNT(*) FROM fact_anime_metrics
WHERE average_score < 0 OR average_score > 100;

-- Check for duplicates
SELECT anime_id, COUNT(*)
FROM dim_anime
WHERE is_current = TRUE
GROUP BY anime_id
HAVING COUNT(*) > 1;
```

---

## 3. ETL Monitoring

### 3.1 Run ETL Pipeline

**Full extraction (100 anime):**
```bash
python scripts/run_etl.py --limit 100
```

**Quick test (10 anime):**
```bash
python scripts/run_etl.py --limit 10
```

**Incremental update (last 24 hours):**
```bash
python scripts/run_etl.py --incremental
```

**Skip similarity computation (faster):**
```bash
python scripts/run_etl.py --limit 50 --skip-similarity
```

### 3.2 ETL Output

The pipeline shows:
- âœ… Extraction progress (anime fetched from API)
- âœ… Transformation results (validation errors if any)
- âœ… Load results (records inserted per table)
- âœ… Similarity computation (scores calculated)
- âœ… Quality checks (passed/failed)

**Example Output:**
```
================================================================================
ETL PIPELINE SUMMARY
================================================================================
Duration: 2.81 seconds
Extracted: 20 records
Transformed: 20 records
Loaded: 103 records
Similarities computed: 70
Quality score: 100.0%
================================================================================
```

### 3.3 Check Last ETL Run

```python
import duckdb
conn = duckdb.connect('warehouse/anime_dw.duckdb')

# Most recent data update
latest = conn.execute("""
    SELECT MAX(snapshot_date) as last_update
    FROM fact_anime_metrics
""").fetchone()

print(f"Last data update: {latest[0]}")
conn.close()
```

---

## 4. Data Quality Checks

### 4.1 Run Quality Checks

```bash
python -c "
from etl.quality.quality_checks import DataQualityChecker

checker = DataQualityChecker('warehouse/anime_dw.duckdb')
checker.add_standard_checks()
results = checker.run_all()
checker.print_report(results)
"
```

**Expected Output:**
```
================================================================================
DATA QUALITY REPORT
================================================================================
Total Checks: 10
Passed: 10
Failed: 0
Quality Score: 100.0%
================================================================================
âœ“ All checks passed!
```

### 4.2 Quality Checks Included

1. **dim_anime.anime_id_not_null** - No null IDs
2. **dim_anime.title_not_null** - All anime have titles
3. **dim_anime.anime_id_unique** - No duplicate anime
4. **fact_anime_metrics.average_score_range** - Scores between 0-100
5. **fact_anime_metrics.popularity_range** - Popularity â‰¥ 0
6. **fact_anime_metrics.anime_key_not_null** - All metrics have anime reference
7. **Referential integrity** - All foreign keys valid
8. **no_duplicate_current_anime** - Only one current version per anime

### 4.3 Custom Quality Check

```python
from etl.quality.quality_checks import DataQualityChecker, CustomSQLCheck

checker = DataQualityChecker('warehouse/anime_dw.duckdb')

# Add custom check
checker.add_check(CustomSQLCheck(
    'high_score_low_popularity',
    """
    SELECT title, averageScore, popularity
    FROM vw_anime_current
    WHERE averageScore > 85 AND popularity < 10000
    """,
    severity='warning'
))

results = checker.run_all()
```

---

## 5. Logs

### 5.1 Log Locations

```
logs/
â”œâ”€â”€ etl/
â”‚   â”œâ”€â”€ run_etl.log          # ETL pipeline execution logs
â”‚   â”œâ”€â”€ warehouse_init.log   # Warehouse initialization logs
â”‚   â””â”€â”€ migration.log        # Data migration logs
â””â”€â”€ app/
    â””â”€â”€ app.log              # Streamlit app logs (if configured)
```

### 5.2 View ETL Logs

**Last 50 lines:**

Linux/Mac:
```bash
tail -50 logs/etl/run_etl.log
```

Windows PowerShell:
```powershell
Get-Content logs/etl/run_etl.log -Tail 50
```

Windows CMD:
```cmd
powershell "Get-Content logs/etl/run_etl.log -Tail 50"
```

**Follow logs in real-time:**

Linux/Mac:
```bash
tail -f logs/etl/run_etl.log
```

Windows PowerShell:
```powershell
Get-Content logs/etl/run_etl.log -Wait -Tail 50
```

**Search for errors:**

Linux/Mac:
```bash
grep -i error logs/etl/run_etl.log
```

Windows PowerShell:
```powershell
Select-String -Path logs/etl/run_etl.log -Pattern "error" -CaseSensitive:$false
```

Windows CMD:
```cmd
findstr /i "error" logs\etl\run_etl.log
```

**View only INFO level:**

Linux/Mac:
```bash
grep INFO logs/etl/run_etl.log | tail -20
```

Windows PowerShell:
```powershell
Select-String -Path logs/etl/run_etl.log -Pattern "INFO" | Select-Object -Last 20
```

### 5.3 Log Levels

- **DEBUG** - Detailed information for diagnosing problems
- **INFO** - General informational messages
- **WARNING** - Something unexpected but not critical
- **ERROR** - Serious problem, some functionality may fail
- **CRITICAL** - Very serious error, program may crash

---

## 6. Streamlit App

### 6.1 Launch App

```bash
streamlit run app.py
```

Open browser: http://localhost:8502

### 6.2 Monitor Through App

The app shows:
- **Home** - Featured anime and statistics
- **Browse** - Filter by genre, sort by score/popularity
- **Top Rated** - Highest scoring anime
- **Most Popular** - Most popular anime
- **Recommendations** - Get similar anime

### 6.3 App Caching

Streamlit caches data for performance. To clear cache:
1. Click **C** in the app
2. Select "Clear cache"
3. Or restart the app

---

## 7. Advanced Monitoring

### 7.1 Database Size

```bash
# Windows
dir warehouse\anime_dw.duckdb

# Linux/Mac
ls -lh warehouse/anime_dw.duckdb
```

### 7.2 Table Statistics

```python
import duckdb
conn = duckdb.connect('warehouse/anime_dw.duckdb')

tables = ['dim_anime', 'fact_anime_metrics', 'fact_anime_similarity']

for table in tables:
    count = conn.execute(f"SELECT COUNT(*) FROM {table}").fetchone()[0]
    print(f"{table:30s} {count:,} rows")

conn.close()
```

### 7.3 Query Performance

```python
import duckdb
import time

conn = duckdb.connect('warehouse/anime_dw.duckdb')

query = "SELECT * FROM vw_anime_current ORDER BY averageScore DESC"

start = time.time()
result = conn.execute(query).fetchall()
duration = time.time() - start

print(f"Query returned {len(result)} rows in {duration:.4f} seconds")
conn.close()
```

### 7.4 Similarity Matrix Analysis

```python
import duckdb
conn = duckdb.connect('warehouse/anime_dw.duckdb')

stats = conn.execute("""
    SELECT
        COUNT(*) as total_scores,
        AVG(similarity_score) as avg_similarity,
        MIN(similarity_score) as min_similarity,
        MAX(similarity_score) as max_similarity
    FROM fact_anime_similarity
""").fetchone()

print(f"Total similarity scores: {stats[0]:,}")
print(f"Average similarity: {stats[1]:.4f}")
print(f"Min similarity: {stats[2]:.4f}")
print(f"Max similarity: {stats[3]:.4f}")

conn.close()
```

---

## 8. Health Check Script

Create a quick health check:

```python
# scripts/health_check.py
import duckdb
from datetime import datetime, timedelta

def health_check():
    conn = duckdb.connect('warehouse/anime_dw.duckdb')

    checks = {
        'database_accessible': False,
        'has_anime': False,
        'has_metrics': False,
        'has_similarities': False,
        'data_fresh': False
    }

    try:
        # Check database accessible
        conn.execute("SELECT 1")
        checks['database_accessible'] = True

        # Check has anime
        anime_count = conn.execute("SELECT COUNT(*) FROM vw_anime_current").fetchone()[0]
        checks['has_anime'] = anime_count > 0

        # Check has metrics
        metrics_count = conn.execute("SELECT COUNT(*) FROM fact_anime_metrics").fetchone()[0]
        checks['has_metrics'] = metrics_count > 0

        # Check has similarities
        sim_count = conn.execute("SELECT COUNT(*) FROM fact_anime_similarity").fetchone()[0]
        checks['has_similarities'] = sim_count > 0

        # Check data freshness (within last 7 days)
        latest = conn.execute("SELECT MAX(snapshot_date) FROM fact_anime_metrics").fetchone()[0]
        if latest:
            checks['data_fresh'] = (datetime.now().date() - latest).days <= 7

    finally:
        conn.close()

    # Print results
    print("HEALTH CHECK RESULTS:")
    for check, status in checks.items():
        symbol = "âœ“" if status else "âœ—"
        print(f"  {symbol} {check}")

    return all(checks.values())

if __name__ == '__main__':
    healthy = health_check()
    exit(0 if healthy else 1)
```

Run it:
```bash
python scripts/health_check.py
```

---

## 9. Troubleshooting

### Issue: No data in warehouse
```bash
python scripts/quick_migrate.py  # Re-run migration
```

### Issue: Similarity scores missing
```bash
python -c "from etl.ml.similarity_engine import compute_similarities; compute_similarities('warehouse/anime_dw.duckdb')"
```

### Issue: Streamlit app not loading data
1. Check database path in app.py
2. Clear Streamlit cache (press 'C' in app)
3. Restart the app

### Issue: ETL failing
1. Check logs (Windows): `powershell "Get-Content logs/etl/run_etl.log -Tail 50"`
   Check logs (Linux/Mac): `tail -50 logs/etl/run_etl.log`
2. Run with smaller limit: `python scripts/run_etl.py --limit 5`
3. Skip similarity: `python scripts/run_etl.py --limit 10 --skip-similarity`

---

## 10. Quick Reference

**Note:** For Windows users, replace `tail -50` with `powershell "Get-Content ... -Tail 50"`

| Task | Command |
|------|---------|
| View anime count | `python -c "import duckdb; print(duckdb.connect('warehouse/anime_dw.duckdb').execute('SELECT COUNT(*) FROM vw_anime_current').fetchone()[0])"` |
| Run ETL | `python scripts/run_etl.py --limit 20` |
| Check quality | `python -c "from etl.quality.quality_checks import DataQualityChecker; c = DataQualityChecker('warehouse/anime_dw.duckdb'); c.add_standard_checks(); c.print_report(c.run_all())"` |
| View logs (Linux/Mac) | `tail -50 logs/etl/run_etl.log` |
| View logs (Windows) | `powershell "Get-Content logs/etl/run_etl.log -Tail 50"` |
| Start app | `streamlit run app.py` |
| Health check | `python scripts/health_check.py` |

---

## Summary

You now have multiple ways to monitor your data warehouse:
- âœ… **Direct SQL queries** for detailed inspection
- âœ… **ETL pipeline output** for load monitoring
- âœ… **Quality checks** for data validation
- âœ… **Logs** for debugging
- âœ… **Streamlit app** for visual exploration
- âœ… **Health checks** for system status

**Current Status:**
- 50 anime in warehouse
- 70 metric snapshots
- 70 similarity scores
- 100% data quality
- All systems operational âœ“
